# ============================================================
# Producción: vLLM con GPU NVIDIA (servidor Debian + RTX 6000)
# Uso: docker compose -f docker-compose.yml -f docker-compose.prod.yml --profile qwen-fast up -d
# ============================================================
services:
  postgres:
    volumes:
      - /opt/ai/postgres:/var/lib/postgresql/data

  litellm:
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - /opt/ai/compose/litellm-active.yml:/app/config.yaml:ro

  # vLLM FAST (Qwen 7B AWQ) — concurrencia 4, rápido
  vllm-fast:
    image: vllm/vllm-openai:v0.5.4
    container_name: vllm-fast
    runtime: nvidia
    ports:
      - "8001:8000"
    environment:
      - HUGGING_FACE_HUB_CACHE=/data/hf-cache
    volumes:
      - /opt/ai/hf-cache:/data/hf-cache
    command:
      [
        "--model","Qwen/Qwen2.5-7B-Instruct-AWQ",
        "--host","0.0.0.0",
        "--port","8000",
        "--dtype","half",
        "--quantization","awq",
        "--max-model-len","4096",
        "--gpu-memory-utilization","0.55",
        "--max-num-seqs","4"
      ]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 120s
    restart: unless-stopped
    profiles: ["qwen-fast"]

  # vLLM QUALITY (Qwen 14B AWQ) — solo cuando lo necesites
  vllm-quality:
    image: vllm/vllm-openai:v0.5.4
    container_name: vllm-quality
    runtime: nvidia
    ports:
      - "8002:8000"
    environment:
      - HUGGING_FACE_HUB_CACHE=/data/hf-cache
    volumes:
      - /opt/ai/hf-cache:/data/hf-cache
    command:
      [
        "--model","Qwen/Qwen2.5-14B-Instruct-AWQ",
        "--host","0.0.0.0",
        "--port","8000",
        "--dtype","half",
        "--quantization","awq",
        "--max-model-len","3072",
        "--gpu-memory-utilization","0.85",
        "--max-num-seqs","1"
      ]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 120s
    restart: unless-stopped
    profiles: ["qwen-quality"]

  # vLLM DEEPSEEK (DeepSeek-R1-Distill-Qwen-32B AWQ) — razonamiento
  vllm-deepseek:
    image: vllm/vllm-openai:v0.5.4
    container_name: vllm-deepseek
    runtime: nvidia
    ports:
      - "8003:8000"
    environment:
      - HUGGING_FACE_HUB_CACHE=/data/hf-cache
    volumes:
      - /opt/ai/hf-cache:/data/hf-cache
    command:
      [
        "--model","casperhansen/deepseek-r1-distill-qwen-32b-awq",
        "--host","0.0.0.0",
        "--port","8000",
        "--dtype","half",
        "--quantization","awq",
        "--max-model-len","4096",
        "--gpu-memory-utilization","0.90",
        "--max-num-seqs","1"
      ]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 180s
    restart: unless-stopped
    profiles: ["deepseek"]

  open-webui:
    volumes:
      - /opt/ai/openwebui-data:/app/backend/data
