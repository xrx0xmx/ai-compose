# ============================================================
# Producción: vLLM con GPU NVIDIA (servidor Debian + RTX 6000)
# Uso: docker compose -f docker-compose.yml -f docker-compose.prod.yml --profile qwen-fast up -d
# ============================================================
services:
  postgres:
    volumes:
      - /opt/ai/postgres:/var/lib/postgresql/data

  litellm:
    depends_on:
      postgres:
        condition: service_healthy
      model-switcher:
        condition: service_healthy
    command: ["--config", "/config/active.yml", "--port", "4000", "--num_workers", "1"]
    volumes:
      - litellm-config:/config

  docker-socket-proxy:
    image: tecnativa/docker-socket-proxy:0.1.1
    container_name: docker-socket-proxy
    environment:
      - CONTAINERS=1
      - POST=1
      - GET=1
      - LOGS=1
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - switcher-net
    restart: unless-stopped

  model-switcher:
    build:
      context: .
      dockerfile: control/Dockerfile
    container_name: model-switcher
    environment:
      - PYTHONDONTWRITEBYTECODE=1
      - MODEL_SWITCHER_TOKEN=${MODEL_SWITCHER_TOKEN:-change_me}
      - MODEL_SWITCHER_DEFAULT=${MODEL_SWITCHER_DEFAULT:-qwen-fast}
      - MODEL_SWITCHER_BIND=0.0.0.0
      - MODEL_SWITCHER_PORT=9000
      - MODEL_SWITCHER_DOCKER_TIMEOUT_SECONDS=${MODEL_SWITCHER_DOCKER_TIMEOUT_SECONDS:-30}
      - MODEL_SWITCHER_HEALTH_TIMEOUT_SECONDS=${MODEL_SWITCHER_HEALTH_TIMEOUT_SECONDS:-480}
      - MODEL_SWITCHER_POLL_INTERVAL_SECONDS=${MODEL_SWITCHER_POLL_INTERVAL_SECONDS:-2}
      - MODEL_SWITCHER_LITELLM_VERIFY_TIMEOUT_SECONDS=${MODEL_SWITCHER_LITELLM_VERIFY_TIMEOUT_SECONDS:-90}
      - MODEL_SWITCHER_LITELLM_MODELS_URL=http://litellm:4000/v1/models
      - MODEL_SWITCHER_LITELLM_KEY=${LITELLM_KEY:-cambiaLAclave}
      - DOCKER_PROXY_URL=http://docker-socket-proxy:2375
      - MODEL_CONFIG_DIR=/config
      - MODEL_TEMPLATE_DIR=/opt/model-configs
      - MODEL_SWITCHER_COMFY_CONTAINER=comfyui
      - MODEL_SWITCHER_COMFY_DEFAULT_TTL_MINUTES=${MODEL_SWITCHER_COMFY_DEFAULT_TTL_MINUTES:-45}
      - MODEL_SWITCHER_COMFY_MAX_TTL_MINUTES=${MODEL_SWITCHER_COMFY_MAX_TTL_MINUTES:-90}
      - MODEL_SWITCHER_MODE_POLL_INTERVAL_SECONDS=${MODEL_SWITCHER_MODE_POLL_INTERVAL_SECONDS:-5}
    depends_on:
      - docker-socket-proxy
    volumes:
      - litellm-config:/config
    networks:
      - default
      - switcher-net
    ports:
      - "127.0.0.1:9000:9000"
    read_only: true
    tmpfs:
      - /tmp
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:9000/health')\""]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # vLLM FAST (Qwen 7B AWQ) — concurrencia 4, rápido
  vllm-fast:
    image: vllm/vllm-openai:v0.5.4
    container_name: vllm-fast
    runtime: nvidia
    ports:
      - "8001:8000"
    environment:
      - HUGGING_FACE_HUB_CACHE=/data/hf-cache
    volumes:
      - /opt/ai/hf-cache:/data/hf-cache
    command:
      [
        "--model","Qwen/Qwen2.5-7B-Instruct-AWQ",
        "--host","0.0.0.0",
        "--port","8000",
        "--dtype","half",
        "--quantization","awq",
        "--max-model-len","4096",
        "--gpu-memory-utilization","0.55",
        "--max-num-seqs","4"
      ]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 120s
    restart: unless-stopped
    profiles: ["qwen-fast"]

  # vLLM QUALITY (Qwen 14B AWQ) — solo cuando lo necesites
  vllm-quality:
    image: vllm/vllm-openai:v0.5.4
    container_name: vllm-quality
    runtime: nvidia
    ports:
      - "8002:8000"
    environment:
      - HUGGING_FACE_HUB_CACHE=/data/hf-cache
    volumes:
      - /opt/ai/hf-cache:/data/hf-cache
    command:
      [
        "--model","Qwen/Qwen2.5-14B-Instruct-AWQ",
        "--host","0.0.0.0",
        "--port","8000",
        "--dtype","half",
        "--quantization","awq",
        "--max-model-len","3072",
        "--gpu-memory-utilization","0.85",
        "--max-num-seqs","1"
      ]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 120s
    restart: unless-stopped
    profiles: ["qwen-quality"]

  # vLLM DEEPSEEK (DeepSeek-R1-Distill-Qwen-14B AWQ) — razonamiento
  vllm-deepseek:
    image: vllm/vllm-openai:v0.6.6.post1
    container_name: vllm-deepseek
    runtime: nvidia
    ports:
      - "8003:8000"
    environment:
      - HUGGING_FACE_HUB_CACHE=/data/hf-cache
    volumes:
      - /opt/ai/hf-cache:/data/hf-cache
    command:
      [
        "--model","casperhansen/deepseek-r1-distill-qwen-14b-awq",
        "--host","0.0.0.0",
        "--port","8000",
        "--dtype","half",
        "--quantization","awq",
        "--max-model-len","2048",
        "--gpu-memory-utilization","0.95",
        "--max-num-seqs","1"
      ]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 180s
    restart: unless-stopped
    profiles: ["deepseek"]

  # vLLM QWEN-MAX (Qwen 2.5-32B AWQ) — máxima calidad
  vllm-qwen32b:
    image: vllm/vllm-openai:v0.5.4
    container_name: vllm-qwen32b
    runtime: nvidia
    ports:
      - "8004:8000"
    environment:
      - HUGGING_FACE_HUB_CACHE=/data/hf-cache
    volumes:
      - /opt/ai/hf-cache:/data/hf-cache
    command:
      [
        "--model","Qwen/Qwen2.5-32B-Instruct-AWQ",
        "--host","0.0.0.0",
        "--port","8000",
        "--dtype","half",
        "--quantization","awq",
        "--max-model-len","2048",
        "--gpu-memory-utilization","0.95",
        "--max-num-seqs","1"
      ]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 180s
    restart: unless-stopped
    profiles: ["qwen-max"]

  # ComfyUI (imagen) — activación bajo demanda para no competir con LLM
  comfyui:
    image: ${COMFYUI_IMAGE:-yanwk/comfyui-boot:cu126-slim}
    container_name: comfyui
    runtime: nvidia
    ports:
      - "8188:8188"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HUGGINGFACE_HUB_CACHE=/data/hf-cache
      - HF_HOME=/data/hf-cache
    volumes:
      - /opt/ai/comfyui-data:/root/ComfyUI
      - /opt/ai/hf-cache:/data/hf-cache
      - ./comfyui/manager-config.ini:/root/ComfyUI/custom_nodes/ComfyUI-Manager/config.ini:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8188/system_stats || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 120s
    restart: unless-stopped
    profiles: ["comfy"]

  open-webui:
    volumes:
      - /opt/ai/openwebui-data:/app/backend/data

  admin-panel:
    build:
      context: ./admin
    container_name: admin-panel
    environment:
      - ADMIN_JWT_SECRET=${ADMIN_JWT_SECRET:-change-this-jwt-secret}
      - MODEL_SWITCHER_URL=http://model-switcher:9000
      - MODEL_SWITCHER_TOKEN=${MODEL_SWITCHER_TOKEN:-change_me}
      - DOCKER_PROXY_URL=http://docker-socket-proxy:2375
      - WEBUI_DB_PATH=/webui-data/webui.db
    volumes:
      - /opt/ai/openwebui-data:/webui-data:ro
    networks:
      - default
      - switcher-net
    ports:
      - "80:8080"
    depends_on:
      model-switcher:
        condition: service_healthy
      docker-socket-proxy:
        condition: service_started
    read_only: true
    tmpfs:
      - /tmp
    cap_drop:
      - ALL
    security_opt:
      - no-new-privileges:true
    restart: unless-stopped

networks:
  switcher-net:
    internal: true
