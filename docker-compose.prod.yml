# ============================================================
# Producción: vLLM con GPU NVIDIA (servidor Debian + RTX 6000)
# Uso: docker compose -f docker-compose.yml -f docker-compose.prod.yml --profile qwen-fast up -d
# ============================================================
services:
  postgres:
    volumes:
      - /opt/ai/postgres:/var/lib/postgresql/data

  litellm:
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - /opt/ai/compose/litellm-active.yml:/app/config.yaml:ro

  # vLLM FAST (Qwen 7B AWQ) — concurrencia 4, rápido
  vllm-fast:
    image: vllm/vllm-openai:v0.5.4
    container_name: vllm-fast
    runtime: nvidia
    ports:
      - "8001:8000"
    environment:
      - HUGGING_FACE_HUB_CACHE=/data/hf-cache
    volumes:
      - /opt/ai/hf-cache:/data/hf-cache
    command:
      [
        "--model","Qwen/Qwen2.5-7B-Instruct-AWQ",
        "--host","0.0.0.0",
        "--port","8000",
        "--dtype","half",
        "--quantization","awq",
        "--max-model-len","4096",
        "--gpu-memory-utilization","0.55",
        "--max-num-seqs","4"
      ]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 120s
    labels:
      - ai.model.id=qwen-fast
      - ai.model.display_name=Qwen 2.5 7B AWQ (Fast)
      - ai.model.profile=qwen-fast
      - ai.model.litellm_config=litellm-config.qwen-fast.yml
    restart: unless-stopped
    profiles: ["qwen-fast"]

  # vLLM QUALITY (Qwen 14B AWQ) — solo cuando lo necesites
  vllm-quality:
    image: vllm/vllm-openai:v0.5.4
    container_name: vllm-quality
    runtime: nvidia
    ports:
      - "8002:8000"
    environment:
      - HUGGING_FACE_HUB_CACHE=/data/hf-cache
    volumes:
      - /opt/ai/hf-cache:/data/hf-cache
    command:
      [
        "--model","Qwen/Qwen2.5-14B-Instruct-AWQ",
        "--host","0.0.0.0",
        "--port","8000",
        "--dtype","half",
        "--quantization","awq",
        "--max-model-len","3072",
        "--gpu-memory-utilization","0.85",
        "--max-num-seqs","1"
      ]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 120s
    labels:
      - ai.model.id=qwen-quality
      - ai.model.display_name=Qwen 2.5 14B AWQ (Quality)
      - ai.model.profile=qwen-quality
      - ai.model.litellm_config=litellm-config.qwen-quality.yml
    restart: unless-stopped
    profiles: ["qwen-quality"]

  # vLLM DEEPSEEK (DeepSeek-R1-Distill-Qwen-14B AWQ) — razonamiento
  vllm-deepseek:
    image: vllm/vllm-openai:v0.6.6.post1
    container_name: vllm-deepseek
    runtime: nvidia
    ports:
      - "8003:8000"
    environment:
      - HUGGING_FACE_HUB_CACHE=/data/hf-cache
    volumes:
      - /opt/ai/hf-cache:/data/hf-cache
    command:
      [
        "--model","casperhansen/deepseek-r1-distill-qwen-14b-awq",
        "--host","0.0.0.0",
        "--port","8000",
        "--dtype","half",
        "--quantization","awq",
        "--max-model-len","2048",
        "--gpu-memory-utilization","0.95",
        "--max-num-seqs","1"
      ]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 180s
    labels:
      - ai.model.id=deepseek-r1
      - ai.model.display_name=DeepSeek R1 Distill Qwen 14B AWQ
      - ai.model.profile=deepseek
      - ai.model.litellm_config=litellm-config.deepseek.yml
    restart: unless-stopped
    profiles: ["deepseek"]

  # vLLM QWEN-MAX (Qwen 2.5-32B AWQ) — máxima calidad
  vllm-qwen32b:
    image: vllm/vllm-openai:v0.5.4
    container_name: vllm-qwen32b
    runtime: nvidia
    ports:
      - "8004:8000"
    environment:
      - HUGGING_FACE_HUB_CACHE=/data/hf-cache
    volumes:
      - /opt/ai/hf-cache:/data/hf-cache
    command:
      [
        "--model","Qwen/Qwen2.5-32B-Instruct-AWQ",
        "--host","0.0.0.0",
        "--port","8000",
        "--dtype","half",
        "--quantization","awq",
        "--max-model-len","2048",
        "--gpu-memory-utilization","0.95",
        "--max-num-seqs","1"
      ]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 180s
    labels:
      - ai.model.id=qwen-max
      - ai.model.display_name=Qwen 2.5 32B AWQ (Max)
      - ai.model.profile=qwen-max
      - ai.model.litellm_config=litellm-config.qwen-max.yml
    restart: unless-stopped
    profiles: ["qwen-max"]

  open-webui:
    volumes:
      - /opt/ai/openwebui-data:/app/backend/data

  model-switcher:
    build:
      context: .
      dockerfile: model-switcher/Dockerfile
    container_name: model-switcher
    environment:
      - MODEL_SWITCHER_ADMIN_TOKEN=${MODEL_SWITCHER_ADMIN_TOKEN:-change-this-admin-token}
      - MODEL_SWITCHER_COMPOSE_DIR=/opt/ai/compose
      - MODEL_SWITCHER_LITELLM_KEY=${LITELLM_KEY:-cambiaLAclave}
      - MODEL_SWITCHER_AUDIT_FILE=/opt/ai/compose/model-switcher-audit.log
      - MODEL_SWITCHER_STATE_FILE=/opt/ai/compose/.active-model.json
      - MODEL_SWITCHER_RATE_LIMIT_PER_MINUTE=5
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /opt/ai/compose:/opt/ai/compose
    depends_on:
      - litellm
      - open-webui
    restart: unless-stopped
    profiles: ["webui"]

  ui-gateway:
    image: nginx:1.27-alpine
    container_name: ui-gateway
    ports:
      - "3000:80"
    volumes:
      - /opt/ai/compose/model-switcher/nginx/default.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      - open-webui
      - model-switcher
    restart: unless-stopped
    profiles: ["webui"]
